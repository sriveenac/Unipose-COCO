{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "This is the python notebook that we use to run the model on the COCO dataset. We use a different version of training annotations of the COCO Dataset to make it compatible with the MPII case so as to set the benchmarks. The second cell from top generates the json file in the format we need after taking as input the actual training annotations file for COCO dataset.",
   "metadata": {
    "cell_id": "00000-0f656aed-18c3-431e-b45d-4589c2a88ea4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 854,
    "execution_start": 1623121083215,
    "source_hash": "ba3e7034",
    "tags": [],
    "cell_id": "00001-50365ed9-b493-497c-a2a9-a6c5f5fc87e3",
    "deepnote_cell_type": "code"
   },
   "source": "import time\nimport torch.optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport torch.utils.data as data\nimport torch.backends.cudnn as cudnn\nimport sys\nimport numpy as np\nimport json\nimport resnet\nimport math\nimport os\n\nfrom PIL import Image\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\n\nimport cv2",
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-dd380471-f5c6-478a-a4e4-6632ed2d9496",
    "deepnote_cell_type": "code"
   },
   "source": "#generate json in format that we need-> only run this once to create json\nimport pandas as pd\nimport json\nwith open('annotations/person_keypoints_val2017.json') as f:\n    data = json.load(f)\ndata.pop('info', None)\ndata.pop('licenses', None)\nimages_data = pd.DataFrame(data['images'])\nimages_data = images_data[['file_name', 'height','width','id']]\nimages_data = images_data.rename(columns = {'id':'image_id'})\nannotations_data = pd.DataFrame(data['annotations'])\nannotations_data = annotations_data.loc[annotations_data.iscrowd == 0 ]\nannotations_data = annotations_data[['segmentation', 'keypoints','image_id']]\nfinal = pd.merge(images_data, annotations_data, on=['image_id'])\nfinal = final.to_dict(orient = 'records')\nwith open('val_coco.json', 'w') as fp:\n    json.dump(final,fp,indent = 2)\n    \nwith open('annotations/person_keypoints_train2017.json') as f:\n    data = json.load(f)\ndata.pop('info', None)\ndata.pop('licenses', None)\nimages_data = pd.DataFrame(data['images'])\nimages_data = images_data[['file_name', 'height','width','id']]\nimages_data = images_data.rename(columns = {'id':'image_id'})\nannotations_data = pd.DataFrame(data['annotations'])\nannotations_data = annotations_data.loc[annotations_data.iscrowd == 0 ]\nannotations_data = annotations_data[['segmentation', 'keypoints','image_id']]\nfinal = pd.merge(images_data, annotations_data, on=['image_id'])\nfinal = final.to_dict(orient = 'records')\nwith open('train_coco.json', 'w') as fp:\n    json.dump(final,fp,indent = 2)",
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1623121085882,
    "source_hash": "ddd44a79",
    "tags": [],
    "cell_id": "00003-6465a158-687c-4218-909a-131b43d66a78",
    "deepnote_cell_type": "code"
   },
   "source": "class Atrous(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size, padding, d, BN):\n        super(Atrous, self).__init__()\n        self.conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=d, bias=False)\n        self.batch = BN(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.batch(self.conv(x))\n        return self.relu(x)\n\n    def _init_weight(self):\n        for mod in self.modules():\n            if isinstance(mod, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(mod.weight)\n            elif isinstance(mod, nn.BatchNorm2d):\n                mod.weight.data.fill_(1)\n                mod.bias.data.zero_()\n            elif isinstance(mod, nn.BatchNorm2d):\n                mod.weight.data.fill_(1)\n                mod.bias.data.zero_()\n\nclass wasp(nn.Module):\n    def __init__(self, bb, output_stride, BN):\n        super(wasp,self).__init__()\n        inplanes = 2048\n        if output_stride == 16:\n            ds = [24, 18, 12,  6]\n        elif output_stride == 8:\n            ds = [48, 36, 24, 12]\n        else: \n            print('build wasp error')\n\n        self.a1 = Atrous(inplanes, 256, 1, padding=0, d=ds[0], BN=BN)\n        self.a2 = Atrous(256, 256, 3, padding=ds[1], d=ds[1], BN=BN)\n        self.a3 = Atrous(256, 256, 3, padding=ds[2], d=ds[2], BN=BN)\n        self.a4 = Atrous(256, 256, 3, padding=ds[3], d=ds[3], BN=BN)\n\n        self.g_avg_pool = nn.Sequential(\n                            nn.AdaptiveAvgPool2d((1, 1)),\n                            nn.Conv2d(inplanes, 256, 1, stride=1, bias=False),\n                            nn.ReLU())\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.conv2 = nn.Conv2d(256,256,1,bias=False)\n        self.bn1 = BN(256)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self._init_weight()\n\n    def forward(self, x):\n        #atrous layers\n        x1 = self.a1(x);x2 = self.a2(x1);x3 = self.a3(x2);x4 = self.a4(x3)\n\n        #conv layers\n        x1 = self.conv2(x1);x2 = self.conv2(x2);x3 = self.conv2(x3);x4 = self.conv2(x4)\n    \n        x1 = self.conv2(x1);x2 = self.conv2(x2);x3 = self.conv2(x3);x4 = self.conv2(x4)\n\n        x5 = self.g_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.relu(self.bn1(x))\n\n        return self.dropout(x)\n        \n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \ndef build_wasp(bb, output_stride, BN):\n    return wasp(bb, output_stride, BN)   ",
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1623121087954,
    "source_hash": "665f13aa",
    "tags": [],
    "cell_id": "00004-3460b770-3a7c-4460-b9a5-b703ecb3d7b7",
    "deepnote_cell_type": "code"
   },
   "source": "class Decoder(nn.Module):\n    def __init__(self, dataset, classes, bb, BN,limbsNum):\n        super(Decoder, self).__init__()\n        ll_inplanes = 256\n        limbsNum = limbsNum\n\n        self.conv1 = nn.Conv2d(ll_inplanes, 48, 1, bias=False)\n        self.batch1 = BN(48)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(2048, 256, 1, bias=False)\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                    BN(256),\n                                    nn.ReLU(),\n                                    nn.Dropout(0.5),\n                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                    BN(256),\n                                    nn.ReLU(),\n                                    nn.Dropout(0.1),\n                                    nn.Conv2d(256, classes+1, kernel_size=1, stride=1))\n\n\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self._init_weight()\n\n\n    def forward(self, x, ll_feat):\n\n        ll_feat = self.conv1(ll_feat)\n        ll_feat = self.batch1(ll_feat)\n        ll_feat = self.relu(ll_feat)\n\n        ll_feat = self.maxpool(ll_feat)\n\n        x = F.interpolate(x, size=ll_feat.size()[2:], mode='bilinear', align_corners=True)\n\n        x = torch.cat((x, ll_feat), dim=1)\n        x = self.last_conv(x)\n\n\n        return x\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef build_decoder(dataset, num_classes, bb, BN,limbsNum):\n    return Decoder(dataset ,num_classes, bb, BN,limbsNum)",
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1623121089962,
    "source_hash": "5d53e3d9",
    "tags": [],
    "cell_id": "00005-80f7cecf-ccf9-4c4e-9314-94234ae41c0f",
    "deepnote_cell_type": "code"
   },
   "source": "def build_bb(bb, output_stride, BN):\n    return resnet.ResNet101(output_stride, BN)",
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1623121091246,
    "source_hash": "2a30a943",
    "tags": [],
    "cell_id": "00006-fed106b0-6e49-4c9b-adf1-e292107c72e1",
    "deepnote_cell_type": "code"
   },
   "source": "class unipose(nn.Module):\n    def __init__(self, dataset, bb = 'resnet', output_stride = 16, num_classes = 21, sync_bn = True, freeze_bn = False, stride = 8):\n        super(unipose,self).__init__()\n        self.stride = stride; self.num_classes = num_classes\n\n        BN = nn.BatchNorm2d\n\n        self.pool_center   = nn.AvgPool2d(kernel_size=9, stride=8, padding=1)\n\n        self.bb            =    build_bb(bb, output_stride, BN)\n        self.wasp          = build_wasp(bb, output_stride, BN)\n        self.decoder       = build_decoder(dataset, num_classes, bb, BN, 13)\n\n        if freeze_bn:\n            self.freeze_bn()\n\n    def forward(self, input):\n        x, ll_feat = self.bb(input)\n        x = self.wasp(x)\n        x = self.decoder(x, ll_feat)\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, SynchronizedBatchNorm2d):\n                m.eval()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def get_1x_lr_params(self):\n        modules = [self.bb]\n        for i in range(len(modules)):\n            for m in modules[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n                        or isinstance(m[1], nn.BatchNorm2d):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n    def get_10x_lr_params(self):\n        modules = [self.aspp, self.decoder]\n        for i in range(len(modules)):\n            for m in modules[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n                        or isinstance(m[1], nn.BatchNorm2d):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n",
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1623121096050,
    "source_hash": "56d84a2",
    "tags": [],
    "cell_id": "00007-7030ee81-5583-46a9-ac55-cc41eee5eea9",
    "deepnote_cell_type": "code"
   },
   "source": "def adjust_learning_rate(optimizer, iters, base_lr, gamma, step_size, p='step', m=[1]):\n    if p == 'fixed':\n        lr = base_lr\n    elif p == 'step':\n        lr = base_lr * (gamma ** (iters // step_size))\n    for i, param_group in enumerate(optimizer.param_groups):\n        param_group['lr'] = lr * m[i]\n    return lr",
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-ccc2ba91-6e37-43d0-81b1-5b0ff1b27ed3",
    "deepnote_cell_type": "code"
   },
   "source": "class Resized(object):\n    \n    def __init__(self, size):\n        assert (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2))\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            self.size = size\n\n    def get_params(img, output_size):\n\n        height, width, _ = img.shape\n        \n        return (output_size[0] * 1.0 / height, output_size[1] * 1.0 / width)\n\n    def __call__(self, img, kpt, center):\n\n        ratio = self.get_params(img, self.size)\n\n        return resize(img, kpt, center, ratio)\n    \ndef normalize(tensor, mean, std):\n    for t, m, s in zip(tensor, mean, std):\n        t.sub_(m).div_(s)\n    return tensor\n\ndef to_tensor(pic):\n    return torch.from_numpy(pic.transpose((2, 0, 1))).float()\n",
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1623121110923,
    "source_hash": "78fa7b50",
    "tags": [],
    "cell_id": "00009-ac1dd40e-1ba9-49f9-a474-e0d24be81dcd",
    "deepnote_cell_type": "code"
   },
   "source": "def gk(size_w, size_h, center_x, center_y, sigma):\n    gridy, gridx = np.mgrid[0:size_h, 0:size_w]\n    D2 = (gridx - center_x) ** 2 + (gridy - center_y) ** 2\n    return np.exp(-D2 / 2.0 / sigma / sigma)\n",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-a65c4284-cff4-4ba6-8267-e9b3fdcb15af",
    "deepnote_cell_type": "code"
   },
   "source": "class coco(data.Dataset):\n    def __init__(self, root_dir, sigma, is_train, transform=None):\n        \n        self.transformer = transform\n        self.is_train    = is_train\n        self.sigma       = sigma\n        self.parts_num   = 17\n        self.stride      = 8\n        self.skeleton = [[16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13],[6,7],[6,8],[7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]]\n        \n        self.labels_dir  = root_dir \n        \n        \n        \n        with open(self.labels_dir) as anno_file:\n            self.anno = json.load(anno_file)\n        \n        if is_train == True:\n            self.img_List = []\n            self.images_dir  = 'train2017/'\n            for idx,val in enumerate(self.anno):\n                self.img_List.append(idx)\n                if idx == 50000:\n                    break\n            print(\"Train images \",len(self.img_List))\n                \n        elif is_train == False: \n            self.img_List = []\n            self.images_dir  = 'val2017/'\n            for idx,val in enumerate(self.anno):\n                self.img_List.append(idx)\n                if idx == 4000:\n                    break\n            print(\"Val images \",len(self.img_List))\n            \n      \n        \n    def __getitem__(self, index):\n        scale = 0.25\n        variable = self.anno[self.img_List[index]]\n\n        img_path  = self.images_dir + variable['file_name'] \n        keyp = [variable['keypoints'][i] for i in range(len(variable['keypoints'])) if i in (0,1,3,4,6,7,9,10,12,13,15,16,18,19,21,22,24,25,27,28,30,31,33,34,36,37,39,40,42,43,45,46,48,49)]\n        keyp_ = [[keyp[i],keyp[i+1]] for i in np.arange(0,34,2)]\n        points   = torch.Tensor(keyp_)\n        center = {}\n        img    = np.array(cv2.resize(cv2.imread(img_path),(368,368)), dtype=np.float32)\n        \n        center = [int(img.shape[0]/2),int(img.shape[1]/2)]\n\n        nParts = 17\n        \n    \n        kpt = points\n            \n        height, width, _ = img.shape\n        heatmap = np.zeros((int(height/self.stride), int(width/self.stride), int(len(kpt)+1)), dtype=np.float32)\n        for i in range(len(kpt)):\n            # resize from 368 to 46\n            x = int(kpt[i][0]) * 1.0 / self.stride\n            y = int(kpt[i][1]) * 1.0 / self.stride\n            heat_map = gk(size_h=int(height/self.stride),size_w=int(width/self.stride), center_x=x, center_y=y, sigma=self.sigma)\n            heat_map[heat_map > 1] = 1\n            heat_map[heat_map < 0.0099] = 0\n            heatmap[:, :, i + 1] = heat_map\n        \n        centermap = np.zeros((int(height/self.stride), int(width/self.stride), 1), dtype=np.float32)\n        center_map = gk(size_h=int(height/self.stride), size_w=int(width/self.stride), center_x=int(center[0]/self.stride), center_y=int(center[1]/self.stride), sigma=3)\n        center_map[center_map > 1] = 1\n        center_map[center_map < 0.0099] = 0\n        centermap[:, :, 0] = center_map\n\n\n        img = normalize(to_tensor(img), [128.0, 128.0, 128.0],\n                                     [256.0, 256.0, 256.0])\n        heatmap   = to_tensor(heatmap)\n        centermap = to_tensor(centermap)\n\n        return img, heatmap, centermap, img_path\n        \n    def __len__(self):\n        return len(self.img_List)        \n    ",
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1623121330389,
    "source_hash": "d6f4d5ba",
    "tags": [],
    "cell_id": "00011-45deba30-fbdc-4d24-bc2b-27e9e51b367e",
    "deepnote_cell_type": "code"
   },
   "source": "def getDataloader(dataset, train_dir, val_dir, sigma, stride, workers, batch_size,*test_dir,):\n    if dataset == 'COCO':\n        train_loader = torch.utils.data.DataLoader(\n                                            coco(train_dir, sigma,is_train =  True,\n                                            transform = Resized(368)),\n                                            batch_size  = batch_size, shuffle=True,num_workers = workers, pin_memory=True)\n    \n        val_loader   = torch.utils.data.DataLoader(\n                                            coco(val_dir, sigma, is_train = False,\n                                            transform = Resized(368)),batch_size = 1, shuffle=True,num_workers = 1, pin_memory=True)\n        return train_loader, val_loader \n",
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1623121361812,
    "source_hash": "3921ef3a",
    "tags": [],
    "cell_id": "00012-d8b43697-f4bd-4054-a6a7-0cc137850e57",
    "deepnote_cell_type": "code"
   },
   "source": "def printAccuracies(mAP, AP, mPCKh, PCKh, mPCK, PCK, dataset):\n    if dataset == \"COCO\":\n        print(\"mPCKh: %.2f%%\" % (mPCKh*100))\n        print(\"PCKhs: %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%, %2.2f%%\"\\\n            % (PCKh[0]*100,PCKh[1]*100,PCKh[2]*100,PCKh[3]*100,PCKh[4]*100,PCKh[5]*100,PCKh[6]*100,PCKh[7]*100,PCKh[8]*100,PCKh[9]*100,\\\n                PCKh[10]*100,PCKh[11]*100,PCKh[12]*100,PCKh[13]*100,PCKh[14]*100,PCKh[15]*100,PCKh[16]*100,PCKh[17]*100))",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1623121577658,
    "source_hash": "1ddd4f7f",
    "tags": [],
    "cell_id": "00013-556566ce-cba7-41b8-96f7-5c64da62c1e2",
    "deepnote_cell_type": "code"
   },
   "source": "def get_kpts(maps, img_h = 368.0, img_w = 368.0):\n\n    maps = maps.clone().cpu().data.numpy()\n    map_6 = maps[0]\n\n    kpts = []\n    for m in map_6[1:]:\n        h, w = np.unravel_index(m.argmax(), m.shape)\n        x = int(w * img_w / m.shape[1])\n        y = int(h * img_h / m.shape[0])\n        kpts.append([x,y])\n    return kpts",
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00014-749414c9-0964-4c2c-a3ed-fa707602f9ea",
    "deepnote_cell_type": "code"
   },
   "source": "def calc_dists(preds, target, normalize):\n    preds  =  preds.astype(np.float32)\n    target = target.astype(np.float32)\n    dists  = np.zeros((preds.shape[1], preds.shape[0]))\n\n    for n in range(preds.shape[0]):\n        for c in range(preds.shape[1]):\n            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n                normed_preds   =  preds[n, c, :] / normalize[n]\n                normed_targets = target[n, c, :] / normalize[n]\n                dists[c, n]    = np.linalg.norm(normed_preds - normed_targets)\n            else:\n                dists[c, n]    = -1\n\n    return dists\n\n\ndef dist_acc(dists, threshold = 0.5):\n    dist_cal     = np.not_equal(dists, -1)\n    num_dist_cal = dist_cal.sum()\n\n    if num_dist_cal > 0:\n        return np.less(dists[dist_cal], threshold).sum() * 1.0 / num_dist_cal\n    else:\n        return -1\n\n\ndef get_max_preds(batch_heatmaps):\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width      = batch_heatmaps.shape[3]\n\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx               = np.argmax(heatmaps_reshaped, 2)\n    maxvals           = np.amax(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx     = idx.reshape((batch_size, num_joints, 1))\n\n    preds   = np.tile(idx, (1,1,2)).astype(np.float32)\n\n    preds[:,:,0] = (preds[:,:,0]) % width\n    preds[:,:,1] = np.floor((preds[:,:,1]) / width)\n\n    pred_mask    = np.tile(np.greater(maxvals, 0.0), (1,1,2))\n    pred_mask    = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n\n    return preds, maxvals\n\n\n\ndef accuracy(output, target, thr_PCK, thr_PCKh, dataset, hm_type='gaussian', threshold=0.5):\n    idx  = list(range(output.shape[1]))\n    #print('idx=', idx)\n    norm = 1.0\n\n    if hm_type == 'gaussian':\n        pred, _   = get_max_preds(output)\n        target, _ = get_max_preds(target)\n\n        h         = output.shape[2]\n        w         = output.shape[3]\n        norm      = np.ones((pred.shape[0], 2)) * np.array([h,w]) / 10\n\n    dists = calc_dists(pred, target, norm)\n\n    acc     = np.zeros((len(idx)))\n    avg_acc = 0\n    cnt     = 0\n    visible = np.zeros((len(idx)))\n\n    for i in range(len(idx)):\n        acc[i] = dist_acc(dists[idx[i]])\n        if acc[i] >= 0:\n            avg_acc = avg_acc + acc[i]\n            cnt    += 1\n            visible[i] = 1\n        else:\n            acc[i] = 0\n\n    avg_acc = avg_acc / cnt if cnt != 0 else 0\n\n    if cnt != 0:\n        acc[0] = avg_acc\n\n    PCKh = np.zeros((len(idx)))\n    avg_PCKh = 0\n\n    if dataset == \"COCO\":\n        headLength = np.linalg.norm(target[0,4,:] - target[0,5,:])\n    elif dataset == \"MPII\":\n        headLength = np.linalg.norm(target[0,9,:] - target[0,10,:])\n\n\n    for i in range(len(idx)):\n        PCKh[i] = dist_acc(dists[idx[i]], thr_PCKh*headLength)\n        if PCKh[i] >= 0:\n            avg_PCKh = avg_PCKh + PCKh[i]\n        else:\n            PCKh[i] = 0\n\n    avg_PCKh = avg_PCKh / cnt if cnt != 0 else 0\n\n    if cnt != 0:\n        PCKh[0] = avg_PCKh\n\n\n    PCK = np.zeros((len(idx)))\n    avg_PCK = 0\n\n    if dataset == \"COCO\":\n        pelvis = [(target[0,12,0]+target[0,13,0])/2, (target[0,12,1]+target[0,13,1])/2]\n        torso  = np.linalg.norm(target[0,13,:] - pelvis)\n        \n    elif dataset == \"MPII\":\n        torso  = np.linalg.norm(target[0,7,0] - target[0,8,0])\n\n    for i in range(len(idx)):\n        PCK[i] = dist_acc(dists[idx[i]], thr_PCK*torso)\n\n        if PCK[i] >= 0:\n            avg_PCK = avg_PCK + PCK[i]\n        else:\n            PCK[i] = 0\n\n    avg_PCK = avg_PCK / cnt if cnt != 0 else 0\n\n    if cnt != 0:\n        PCK[0] = avg_PCK\n\n\n    return acc, PCK, PCKh, cnt, pred, visible\n",
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1623122069344,
    "source_hash": "134010fc",
    "tags": [],
    "cell_id": "00015-ee3921de-0d49-4075-a2ac-f75a4256b985",
    "deepnote_cell_type": "code"
   },
   "source": "class Trainer(object):\n    def __init__(self, pretrained,dataset,train_dir,val_dir,model_name,model_arch = 'unipose'):\n\n        #hyperparameters\n        self.train_dir    = \"\"\n        self.val_dir      = \"\"\n        self.dataset      = \"COCO\"\n        self.pretrained = pretrained\n        self.model_name = model_name\n        self.model_arch = model_arch\n\n        self.workers      = 1\n        self.weight_decay = 0.005\n        self.momentum     = 0.9\n        self.batch_size   = 8\n        self.lr           = 0.0005\n        self.gamma        = 0.333\n        self.step_size    = 13275\n        self.sigma        = 3\n        self.stride       = 8\n        cudnn.benchmark   = True\n    \n        if self.dataset == \"COCO\":\n            self.numClasses  = 17\n        self.train_loader, self.val_loader = getDataloader(self.dataset, 'train_coco.json','val_coco.json', self.sigma, self.stride, self.workers, self.batch_size)\n        model = unipose(self.dataset, num_classes=self.numClasses,bb='resnet',output_stride=16,sync_bn=True,freeze_bn=False, stride=self.stride)\n        self.model  = model.cuda()\n        self.criterion   = nn.MSELoss().cuda()\n        self.optimizer   = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        self.best_model  = 12345678.9\n        self.iters       = 0\n        \n        if self.pretrained is not None:\n            checkpoint = torch.load(self.pretrained)\n            p = checkpoint['state_dict']\n\n            state_dict = self.model.state_dict()\n            model_dict = {}\n\n            for k,v in p.items():\n                if k in state_dict:\n                    model_dict[k] = v\n\n            state_dict.update(model_dict)\n            self.model.load_state_dict(state_dict)\n            \n    def training(self, epoch):\n        train_loss = 0.0\n        self.model.train()\n        print(\"epoch \" + str(epoch) + ':') \n        tbar = tqdm(self.train_loader)\n        for i, (input, heatmap, centermap, img_path) in enumerate(tbar):\n            learning_rate = adjust_learning_rate(self.optimizer, self.iters, self.lr, policy='step',\n                                                 gamma=self.gamma, step_size=self.step_size)\n\n            input_var     =     input.cuda()\n            heatmap_var   =    heatmap.cuda()\n            self.optimizer.zero_grad()\n            heat = self.model(input_var)\n            loss_heat   = self.criterion(heat,  heatmap_var)\n            loss = loss_heat\n            train_loss += loss_heat.item()\n            loss.backward()\n            self.optimizer.step()\n            tbar.set_description('Train loss: %.6f' % (train_loss / ((i + 1)*self.batch_size)))\n            self.iters += 1\n        \n           \n    def validation(self, epoch):\n        self.model.eval()\n        tbar = tqdm(self.val_loader, desc='\\r')\n        val_loss = 0.0\n        \n        AP    = np.zeros(self.numClasses+1)\n        PCK   = np.zeros(self.numClasses+1)\n        PCKh  = np.zeros(self.numClasses+1)\n        count = np.zeros(self.numClasses+1)\n\n        cnt = 0\n        for i, (input, heatmap, centermap, img_path) in enumerate(tbar):\n\n            cnt += 1\n\n            input_var     =      input.cuda()\n            heatmap_var   =    heatmap.cuda()\n            self.optimizer.zero_grad()\n\n            heat = self.model(input_var)\n            loss_heat   = self.criterion(heat,  heatmap_var)\n\n            loss = loss_heat\n\n            val_loss += loss_heat.item()\n\n            tbar.set_description('Val loss: %.6f' % (val_loss / ((i + 1)*self.batch_size)))\n\n            acc, acc_PCK, acc_PCKh, cnt, pred, visible = accuracy(heat.detach().cpu().numpy(), heatmap_var.detach().cpu().numpy(),0.2,0.5, self.dataset)\n\n            AP[0]     = (AP[0]  *i + acc[0])      / (i + 1)\n            PCK[0]    = (PCK[0] *i + acc_PCK[0])  / (i + 1)\n            PCKh[0]   = (PCKh[0]*i + acc_PCKh[0]) / (i + 1)\n            for j in range(1,self.numClasses+1):\n                if visible[j] == 1:\n                    AP[j]     = (AP[j]  *count[j] + acc[j])      / (count[j] + 1)\n                    PCK[j]    = (PCK[j] *count[j] + acc_PCK[j])  / (count[j] + 1)\n                    PCKh[j]   = (PCKh[j]*count[j] + acc_PCKh[j]) / (count[j] + 1)\n                    count[j] += 1\n\n            mAP     =   AP[1:].sum()/(self.numClasses)\n            mPCK    =  PCK[1:].sum()/(self.numClasses)\n            mPCKh   = PCKh[1:].sum()/(self.numClasses)\n\n        printAccuracies(mAP, AP, mPCKh, PCKh, mPCK, PCK, self.dataset)",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1623114480262,
    "source_hash": "d38fa124",
    "tags": [],
    "cell_id": "00016-d10b1877-146f-4b06-b242-f6376bd37a43",
    "deepnote_cell_type": "code"
   },
   "source": "#final 10 epoch run\nstarter_epoch =    0\nepochs        =  10\ntrainer = Trainer(None,'COCO', '', '', None, 'unipose' )\nfor epoch in range(starter_epoch, epochs):\n    trainer.training(epoch)\n    trainer.validation(epoch)",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train images  50001\nVal images  4001\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "epoch 0:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000807: 100%|██████████| 6251/6251 [23:51<00:00,  4.37it/s]  \nVal loss: 0.000482: 100%|██████████| 4001/4001 [01:34<00:00, 42.38it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.70%\nPCKhs: 25.32%, 65.29%, 60.84%, 64.29%, 50.60%, 61.04%, 59.29%, 59.54%, 56.08%, 55.85%, 54.67%, 54.44%, 56.23%, 55.72%, 53.01%, 53.05%, 52.50%, 51.54%\nepoch 1:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000476: 100%|██████████| 6251/6251 [23:27<00:00,  4.44it/s]  \nVal loss: 0.000457: 100%|██████████| 4001/4001 [01:24<00:00, 47.28it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.69%\nPCKhs: 25.32%, 65.29%, 60.74%, 64.29%, 50.48%, 61.04%, 59.29%, 59.54%, 56.08%, 55.85%, 54.67%, 54.44%, 56.23%, 55.72%, 53.01%, 53.05%, 52.50%, 51.54%\nepoch 2:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000472: 100%|██████████| 6251/6251 [22:31<00:00,  4.63it/s]  \nVal loss: 0.000580: 100%|██████████| 4001/4001 [01:23<00:00, 48.15it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.95%\nPCKhs: 25.37%, 65.45%, 60.93%, 64.37%, 51.20%, 61.13%, 59.63%, 60.07%, 56.61%, 56.09%, 55.34%, 55.04%, 56.39%, 56.03%, 52.90%, 53.05%, 52.50%, 51.40%\nepoch 3:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000463: 100%|██████████| 6251/6251 [22:50<00:00,  4.56it/s]  \nVal loss: 0.000464: 100%|██████████| 4001/4001 [01:26<00:00, 46.29it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.71%\nPCKhs: 25.33%, 65.29%, 60.74%, 64.29%, 50.60%, 61.13%, 59.22%, 59.54%, 56.08%, 55.77%, 54.58%, 54.36%, 56.23%, 55.72%, 53.01%, 53.05%, 52.64%, 51.83%\nepoch 4:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000459: 100%|██████████| 6251/6251 [22:44<00:00,  4.58it/s]  \nVal loss: 0.000488: 100%|██████████| 4001/4001 [01:22<00:00, 48.58it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 57.03%\nPCKhs: 25.44%, 65.61%, 61.31%, 64.64%, 51.57%, 61.62%, 59.77%, 60.00%, 56.34%, 56.01%, 54.96%, 54.70%, 56.31%, 55.72%, 53.12%, 53.16%, 52.92%, 51.69%\nepoch 5:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000456: 100%|██████████| 6251/6251 [23:20<00:00,  4.46it/s]  \nVal loss: 0.000532: 100%|██████████| 4001/4001 [01:26<00:00, 46.02it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.86%\nPCKhs: 25.35%, 65.21%, 60.65%, 64.11%, 50.60%, 60.94%, 59.36%, 59.74%, 56.43%, 56.01%, 54.96%, 54.70%, 56.47%, 56.11%, 53.23%, 53.27%, 52.78%, 52.11%\nepoch 6:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000454: 100%|██████████| 6251/6251 [23:20<00:00,  4.46it/s]  \nVal loss: 0.000521: 100%|██████████| 4001/4001 [01:26<00:00, 46.18it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.76%\nPCKhs: 25.36%, 65.29%, 60.84%, 64.11%, 50.48%, 61.04%, 59.50%, 59.67%, 56.26%, 56.09%, 54.77%, 54.62%, 56.23%, 55.72%, 53.01%, 53.05%, 52.50%, 51.69%\nepoch 7:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000452: 100%|██████████| 6251/6251 [23:17<00:00,  4.47it/s]  \nVal loss: 0.000531: 100%|██████████| 4001/4001 [01:25<00:00, 46.54it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.95%\nPCKhs: 25.40%, 65.61%, 61.22%, 64.64%, 51.08%, 61.23%, 59.50%, 59.67%, 56.52%, 56.01%, 54.86%, 54.44%, 56.39%, 55.80%, 53.34%, 53.16%, 52.64%, 52.11%\nepoch 8:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000451: 100%|██████████| 6251/6251 [22:45<00:00,  4.58it/s]\nVal loss: 0.000482: 100%|██████████| 4001/4001 [01:32<00:00, 43.42it/s]\n  0%|          | 0/6251 [00:00<?, ?it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 56.96%\nPCKhs: 25.45%, 65.77%, 61.41%, 64.73%, 51.08%, 61.43%, 59.63%, 59.80%, 56.26%, 56.41%, 54.77%, 54.79%, 56.31%, 55.72%, 53.01%, 53.05%, 52.50%, 51.69%\nepoch 9:\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Train loss: 0.000450: 100%|██████████| 6251/6251 [22:46<00:00,  4.58it/s]  \nVal loss: 0.000511: 100%|██████████| 4001/4001 [01:28<00:00, 45.17it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mPCKh: 57.06%\nPCKhs: 25.47%, 65.69%, 61.31%, 64.64%, 51.20%, 61.52%, 59.56%, 59.74%, 56.52%, 56.17%, 55.24%, 54.96%, 56.55%, 55.80%, 53.23%, 53.16%, 52.78%, 51.97%\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=330e0e44-deeb-47a0-bf8a-9dcc20db6494' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "2c48e1c9-60a1-472d-8578-0a9fa6d4e77f",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 }
}